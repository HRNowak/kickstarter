{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3 Selenium Multi-Scraper","provenance":[],"toc_visible":true,"mount_file_id":"1T3SfIDkYpaPS7gWDkN6nLoSTN-GSMXVI","authorship_tag":"ABX9TyPKRafu6BQFX4YZk8QBoA6r"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Y-FeJsdyWMOQ","colab_type":"code","outputId":"176478e3-1974-4821-85a6-16114f517dec","executionInfo":{"status":"ok","timestamp":1589292957046,"user_tz":-60,"elapsed":27730,"user":{"displayName":"Hernán Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":819}},"source":["#intalling chromedriver on computer instance \n","!apt install chromium-chromedriver\n","\n","#Installing selenium library\n","!pip install selenium"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n","Suggested packages:\n","  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n","The following NEW packages will be installed:\n","  chromium-browser chromium-browser-l10n chromium-chromedriver\n","  chromium-codecs-ffmpeg-extra\n","0 upgraded, 4 newly installed, 0 to remove and 29 not upgraded.\n","Need to get 77.2 MB of archives.\n","After this operation, 264 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 81.0.4044.122-0ubuntu0.18.04.1 [1,095 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 81.0.4044.122-0ubuntu0.18.04.1 [68.8 MB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 81.0.4044.122-0ubuntu0.18.04.1 [3,230 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 81.0.4044.122-0ubuntu0.18.04.1 [4,070 kB]\n","Fetched 77.2 MB in 5s (16.4 MB/s)\n","Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n","(Reading database ... 144429 files and directories currently installed.)\n","Preparing to unpack .../chromium-codecs-ffmpeg-extra_81.0.4044.122-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-codecs-ffmpeg-extra (81.0.4044.122-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-browser.\n","Preparing to unpack .../chromium-browser_81.0.4044.122-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-browser (81.0.4044.122-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-browser-l10n.\n","Preparing to unpack .../chromium-browser-l10n_81.0.4044.122-0ubuntu0.18.04.1_all.deb ...\n","Unpacking chromium-browser-l10n (81.0.4044.122-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-chromedriver.\n","Preparing to unpack .../chromium-chromedriver_81.0.4044.122-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-chromedriver (81.0.4044.122-0ubuntu0.18.04.1) ...\n","Setting up chromium-codecs-ffmpeg-extra (81.0.4044.122-0ubuntu0.18.04.1) ...\n","Setting up chromium-browser (81.0.4044.122-0ubuntu0.18.04.1) ...\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-chromedriver (81.0.4044.122-0ubuntu0.18.04.1) ...\n","Setting up chromium-browser-l10n (81.0.4044.122-0ubuntu0.18.04.1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for mime-support (3.60ubuntu1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Collecting selenium\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n","\u001b[K     |████████████████████████████████| 911kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n","Installing collected packages: selenium\n","Successfully installed selenium-3.141.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r6D94sCiYTBe","colab_type":"code","colab":{}},"source":["#Importing webdriver and choosing settings compatible with Google Colab\n","from selenium import webdriver\n","chrome_options = webdriver.ChromeOptions()\n","chrome_options.add_argument('--headless')\n","chrome_options.add_argument('--no-sandbox')\n","chrome_options.add_argument('--disable-dev-shm-usage')\n","\n","#setting driver\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uou178p4sXBz","colab_type":"code","outputId":"498d92bc-9ff5-4a84-f559-eb738c6a4f08","executionInfo":{"status":"ok","timestamp":1589292968878,"user_tz":-60,"elapsed":39546,"user":{"displayName":"Hernán Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":360}},"source":["# Other Libraries\n","from bs4 import BeautifulSoup\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import re\n","from time import sleep\n","!pip install vaderSentiment\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","!pip install jsonlines\n","import jsonlines\n","!pip install ipython-autotime\n","%load_ext autotime"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting vaderSentiment\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/a3/1218a3b5651dbcba1699101c84e5c84c36cbba360d9dbf29f2ff18482982/vaderSentiment-3.3.1-py2.py3-none-any.whl (125kB)\n","\r\u001b[K     |██▋                             | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n","\u001b[?25hInstalling collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.1\n","Collecting jsonlines\n","  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n","Installing collected packages: jsonlines\n","Successfully installed jsonlines-1.2.0\n","Collecting ipython-autotime\n","  Downloading https://files.pythonhosted.org/packages/e6/f9/0626bbdb322e3a078d968e87e3b01341e7890544de891d0cb613641220e6/ipython-autotime-0.1.tar.bz2\n","Building wheels for collected packages: ipython-autotime\n","  Building wheel for ipython-autotime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ipython-autotime: filename=ipython_autotime-0.1-cp36-none-any.whl size=1832 sha256=5d204bd054cfbf2d899f39d27ec61eda2277f5c5ecccd8e64247d810698b904e\n","  Stored in directory: /root/.cache/pip/wheels/d2/df/81/2db1e54bc91002cec40334629bc39cfa86dff540b304ebcd6e\n","Successfully built ipython-autotime\n","Installing collected packages: ipython-autotime\n","Successfully installed ipython-autotime-0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-_ekmfWvtYu-","colab_type":"text"},"source":["# Scraper"]},{"cell_type":"code","metadata":{"id":"-VTVY1AxskT6","colab_type":"code","outputId":"ac72f848-13ac-48a3-8e1d-5ab1f67f9117","executionInfo":{"status":"ok","timestamp":1589292968879,"user_tz":-60,"elapsed":39532,"user":{"displayName":"Hernán Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["class DataScraper:\n","  def __init__(self,list_of_urls,file_path):\n","    self.to_scrape = list_of_urls\n","    self.data_destination = file_path\n","    self.load()\n","\n","  def load(self): ##NEEDS UPDATING USING JSONLINES\n","    try:\n","      with jsonlines.open(file,'r') as reader:\n","        urls = 0\n","        found = 0\n","        for obj in reader:\n","          urls += 1\n","          key = list(obj.keys())[0]\n","          try:\n","            self.to_scrape.remove(key)\n","            found += 1\n","          except:\n","            pass\n","        print(f'{urls+1} projects already stored. {found} removed from list of URLs provided. {len(self.to_scrape)} projects remaining.')\n","    except:\n","      print(f'No previously recorded projects.{len(self.to_scrape)} projects remaining.')\n","\n","  def scrape(self):\n","    driver = webdriver.Chrome('chromedriver',options=chrome_options) #creates intance of webdriver\n","    sentiment_analyser = SentimentIntensityAnalyzer() #creates intance of vader sentiment analyser\n","    scraped = [] #creates empty list to later append individual {projects}\n","    errors = 0\n","    print(f'{len(self.to_scrape)} to be scraped') \n","    for x,url in enumerate(self.to_scrape): \n","      line = {url:{}} #empty project\n","      try:\n","        driver.get(url)\n","        tries = 4\n","        skipped = 0\n","        while tries > 0:\n","          sleep(2)\n","          soup = BeautifulSoup(driver.page_source, 'lxml')\n","          line[url]['r_levels'] = self.r_levels(soup)\n","          line[url]['n_videos'] = self.n_videos(soup)\n","          line[url]['n_imgs'], line[url]['n_gifs'] = self.imgs_and_gifs(soup)\n","          line[url]['story_l'],line[url]['story_s'] = self.l_story(soup,sentiment_analyser) #ADD EXCEPTION if len == 0!\n","          if np.isnan(line[url]['story_l']):\n","            tries = tries - 1\n","            if tries == 0:\n","              errors += 1\n","            continue\n","          line[url]['r&c_l'],line[url]['r&c_s'] = self.l_rc(soup,sentiment_analyser)\n","          line[url]['env_l'],line[url]['env_s'] = self.l_env(soup,sentiment_analyser)\n","          line[url]['faqs'] = self.get_faqs(soup)\n","          line[url]['updates'] = self.get_updates(soup)\n","          tries = 0\n","          scraped.append(line)\n","        if (x+1)%50 == 0:\n","          print(f'{x+1} urls scraped since start. {errors} skipped. Saving...')\n","          self.save(scraped)\n","          scraped = []\n","          errors = 0\n","      except:\n","          pass\n","    self.save(scraped)\n","    driver.close()\n","\n","    print(f'Scraping completed. {x+1} url/s scraped.')\n","\n","  def r_levels(self,soup):\n","    values = []\n","    try:\n","      reward_boxes = soup.find_all('div',{'class':'pledge__info'})\n","      for box in reward_boxes:\n","        try:\n","          value =re.search(r'\\d+',box.find('span',{'class':'money'}).text)\n","          values.append(value.group(0))\n","        except:\n","          pass\n","    except:\n","      pass\n","    return values\n","\n","  def n_videos(self,soup):\n","    videos = 0\n","    try:\n","      found = soup.find_all('video')\n","      videos += len(found)\n","    except:\n","      pass\n","    try:\n","      found = soup.find_all('iframe',{'class':\"embedly-card\"})\n","      videos += len(found)\n","    except:\n","      pass\n","    return videos\n","\n","  def get_faqs(self,soup):\n","    try:\n","      faqs = soup.find('a',{'id':'faq-emoji'}).get('emoji-data')\n","      return int(faqs)\n","    except:\n","      return np.NaN\n","\n","\n","  def get_updates(self,soup):\n","    try:\n","      faqs = soup.find('a',{'id':'updates-emoji'}).get('emoji-data')\n","      return int(faqs)\n","    except:\n","      return np.NaN\n","\n","  def imgs_and_gifs(self,soup):\n","    img = 0\n","    gif = 0\n","    try:\n","      media = soup.find_all('img')\n","      for element in media:\n","        if '.gif' in element.get('src').lower():\n","          gif += 1\n","        else:\n","          img += 1\n","    except:\n","      pass\n","    return img,gif\n","\n","  def l_story(self,soup,sentiment_analyser):\n","    try:\n","      content = soup.find('div',{'class':'rte__content'}).text\n","      sentiment = sentiment_analyser.polarity_scores(content)\n","      count = self.count_words(content)\n","      if count == 0:\n","        return np.NaN,(np.NaN,np.NaN,np.NaN)\n","      else: \n","        return count, (sentiment['neg'],sentiment['pos'],sentiment['compound'])\n","    except:\n","      return np.NaN,(np.NaN,np.NaN,np.NaN)\n","\n","  def l_rc(self,soup,sentiment_analyser):\n","    try:\n","      content = soup.find('div',{'id':'risksAndChallenges'}).text\n","      sentiment = sentiment_analyser.polarity_scores(content)\n","      return self.count_words(content) - 8, (sentiment['neg'],sentiment['pos'],sentiment['compound'])\n","    except:\n","      return np.NaN,(np.NaN,np.NaN,np.NaN)\n","\n","  def l_env(self,soup,sentiment_analyser):\n","    try:\n","      content = soup.find('div',{'id':'environmentalCommitments'}).text\n","      sentiment = sentiment_analyser.polarity_scores(content)\n","      return self.count_words(content),(sentiment['neg'],sentiment['pos'],sentiment['compound'])\n","    except:\n","      return np.NaN,(np.NaN,np.NaN,np.NaN)\n","\n","  def save(self,scraped):\n","    print('Saving...')\n","    with jsonlines.open(self.data_destination, mode='a') as writer:\n","            writer.write_all(scraped)\n","    print('Save completed!')\n","\n","\n","  def count_words(self,text):\n","    tokens = re.findall(r'(\\d+(\\,\\d+)*(\\.?\\d+)*(st|\\'s|s|rd|th|nd)*|\\w+(\\-\\w+)*(\\'s)*)',text)\n","    return len(tokens)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["time: 203 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Yu4ME8Jgpr8r","colab_type":"text"},"source":["# URL List"]},{"cell_type":"code","metadata":{"id":"M7pcH6TWZ1Zn","colab_type":"code","outputId":"9f40e42a-1e65-4e44-9e7e-de7fd63566d7","executionInfo":{"status":"ok","timestamp":1589292968879,"user_tz":-60,"elapsed":39521,"user":{"displayName":"Hernán Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["with open('/content/drive/My Drive/Data Science Projects/Kickstarter/Files/clean_data.csv','r') as file:\n","  urls = []\n","  for line in file.readlines()[66000:99000]:\n","    url = line.split('\\t')[24]\n","    if '?' in url:\n","      urls.append(url.split('?')[0])\n","    else:\n","      urls.append(url)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["time: 800 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6qgfXRWe6IVX","colab_type":"code","outputId":"24ace763-2f6d-484f-984f-c4e09ff6b4cb","executionInfo":{"status":"ok","timestamp":1589292972912,"user_tz":-60,"elapsed":43545,"user":{"displayName":"Hernán Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["file = '/content/drive/My Drive/Data Science Projects/Kickstarter/Files/Scraping/projects3.json'\n","scraper = DataScraper(urls,file)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["11008 projects already stored. 11007 removed from list of URLs provided. 21993 projects remaining.\n","time: 3.73 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G3Q6VeRqbyd-","colab_type":"code","outputId":"b3454261-28f2-49d3-dd86-9ee1a705d65c","executionInfo":{"status":"ok","timestamp":1588932500379,"user_tz":-60,"elapsed":76066,"user":{"displayName":"Hernán Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["scraper.scrape()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["21993 to be scraped\n","50 urls scraped since start. 1 skipped. Saving...\n","Saving...\n","Save completed!\n","100 urls scraped since start. 0 skipped. Saving...\n","Saving...\n","Save completed!\n","150 urls scraped since start. 2 skipped. Saving...\n","Saving...\n","Save completed!\n","200 urls scraped since start. 0 skipped. Saving...\n","Saving...\n","Save completed!\n","250 urls scraped since start. 33 skipped. Saving...\n","Saving...\n","Save completed!\n"],"name":"stdout"}]}]}