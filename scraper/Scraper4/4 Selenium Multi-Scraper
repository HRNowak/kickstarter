{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4 Selenium Multi-Scraper","provenance":[],"mount_file_id":"1Z0um2ubOdxSV_Mw5J7urs35AmFi_6T6F","authorship_tag":"ABX9TyPDJrfM3B4jsc4Hg79XRd7d"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Y-FeJsdyWMOQ","colab_type":"code","outputId":"3e6a87d0-3ab0-484a-9054-8cfd72acce59","executionInfo":{"status":"ok","timestamp":1589272430243,"user_tz":-60,"elapsed":5402,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["#intalling chromedriver on computer instance \n","!apt install chromium-chromedriver\n","\n","#Installing selenium library\n","!pip install selenium"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","chromium-chromedriver is already the newest version (81.0.4044.122-0ubuntu0.18.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n","Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n","time: 4.99 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r6D94sCiYTBe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"918eb7f8-703a-4b0e-89ee-de94a9b4d0d0","executionInfo":{"status":"ok","timestamp":1589272430244,"user_tz":-60,"elapsed":5393,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}}},"source":["#Importing webdriver and choosing settings compatible with Google Colab\n","from selenium import webdriver\n","chrome_options = webdriver.ChromeOptions()\n","chrome_options.add_argument('--headless')\n","chrome_options.add_argument('--no-sandbox')\n","chrome_options.add_argument('--disable-dev-shm-usage')\n","\n","#setting driver\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["time: 4.57 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Uou178p4sXBz","colab_type":"code","outputId":"7def410c-9908-41f0-e768-d7a968382611","executionInfo":{"status":"ok","timestamp":1589272437665,"user_tz":-60,"elapsed":12807,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# Other Libraries\n","from bs4 import BeautifulSoup\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import re\n","from time import sleep\n","!pip install vaderSentiment\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","!pip install jsonlines\n","import jsonlines\n","!pip install ipython-autotime\n","%load_ext autotime"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.6/dist-packages (3.3.1)\n","Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (1.2.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n","Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.6/dist-packages (0.1)\n","The autotime extension is already loaded. To reload it, use:\n","  %reload_ext autotime\n","time: 7.43 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-_ekmfWvtYu-","colab_type":"text"},"source":["# Scraper"]},{"cell_type":"code","metadata":{"id":"-VTVY1AxskT6","colab_type":"code","outputId":"b52d0cbe-9cf3-4ec8-e9bf-2d3c3f0c9193","executionInfo":{"status":"ok","timestamp":1589272445190,"user_tz":-60,"elapsed":657,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["class DataScraper:\n","  def __init__(self,list_of_urls,file_path):\n","    self.to_scrape = list_of_urls\n","    self.data_destination = file_path\n","    self.load()\n","\n","  def load(self): ##NEEDS UPDATING USING JSONLINES\n","    try:\n","      with jsonlines.open(file,'r') as reader:\n","        urls = 0\n","        found = 0\n","        for obj in reader:\n","          urls += 1\n","          key = list(obj.keys())[0]\n","          try:\n","            self.to_scrape.remove(key)\n","            found += 1\n","          except:\n","            pass\n","        print(f'{urls+1} projects already stored. {found} removed from list of URLs provided. {len(self.to_scrape)} projects remaining.')\n","    except:\n","      print(f'No previously recorded projects.{len(self.to_scrape)} projects remaining.')\n","\n","  def scrape(self):\n","    driver = webdriver.Chrome('chromedriver',options=chrome_options) #creates intance of webdriver\n","    sentiment_analyser = SentimentIntensityAnalyzer() #creates intance of vader sentiment analyser\n","    scraped = [] #creates empty list to later append individual {projects}\n","    errors = 0\n","    print(f'{len(self.to_scrape)} to be scraped') \n","    for x,url in enumerate(self.to_scrape): \n","      line = {url:{}} #empty project\n","      try:\n","        driver.get(url)\n","        tries = 4\n","        while tries > 0:\n","          sleep(2)\n","          soup = BeautifulSoup(driver.page_source, 'lxml')\n","          line[url]['r_levels'] = self.r_levels(soup)\n","          line[url]['n_videos'] = self.n_videos(soup)\n","          line[url]['n_imgs'], line[url]['n_gifs'] = self.imgs_and_gifs(soup)\n","          line[url]['story_l'],line[url]['story_s'] = self.l_story(soup,sentiment_analyser) #ADD EXCEPTION if len == 0!\n","          if np.isnan(line[url]['story_l']):\n","            tries = tries - 1\n","            if tries == 0:\n","              errors += 1\n","            continue\n","          line[url]['r&c_l'],line[url]['r&c_s'] = self.l_rc(soup,sentiment_analyser)\n","          line[url]['env_l'],line[url]['env_s'] = self.l_env(soup,sentiment_analyser)\n","          line[url]['faqs'] = self.get_faqs(soup)\n","          line[url]['updates'] = self.get_updates(soup)\n","          tries = 0\n","          scraped.append(line)\n","        if (x+1)%50 == 0:\n","          print(f'{x+1} urls scraped since start. Saving...')\n","          self.save(scraped)\n","          scraped = []\n","          errors = 0\n","      except:\n","        print(f'{x+1} urls scraped since start. {errors} errors. Saving...')\n","    self.save(scraped)\n","    driver.close()\n","\n","    print(f'Scraping completed. {x+1} url/s scraped.')\n","\n","  def r_levels(self,soup):\n","    values = []\n","    try:\n","      reward_boxes = soup.find_all('div',{'class':'pledge__info'})\n","      for box in reward_boxes:\n","        try:\n","          value =re.search(r'\\d+',box.find('span',{'class':'money'}).text)\n","          values.append(value.group(0))\n","        except:\n","          pass\n","    except:\n","      pass\n","    return values\n","\n","  def n_videos(self,soup):\n","    videos = 0\n","    try:\n","      found = soup.find_all('video')\n","      videos += len(found)\n","    except:\n","      pass\n","    try:\n","      found = soup.find_all('iframe',{'class':\"embedly-card\"})\n","      videos += len(found)\n","    except:\n","      pass\n","    return videos\n","\n","  def get_faqs(self,soup):\n","    try:\n","      faqs = soup.find('a',{'id':'faq-emoji'}).get('emoji-data')\n","      return int(faqs)\n","    except:\n","      return np.NaN\n","\n","\n","  def get_updates(self,soup):\n","    try:\n","      faqs = soup.find('a',{'id':'updates-emoji'}).get('emoji-data')\n","      return int(faqs)\n","    except:\n","      return np.NaN\n","\n","  def imgs_and_gifs(self,soup):\n","    img = 0\n","    gif = 0\n","    try:\n","      media = soup.find_all('img')\n","      for element in media:\n","        if '.gif' in element.get('src').lower():\n","          gif += 1\n","        else:\n","          img += 1\n","    except:\n","      pass\n","    return img,gif\n","\n","  def l_story(self,soup,sentiment_analyser):\n","    try:\n","      content = soup.find('div',{'class':'rte__content'}).text\n","      sentiment = sentiment_analyser.polarity_scores(content)\n","      count = self.count_words(content)\n","      if count == 0:\n","        return np.NaN,(np.NaN,np.NaN,np.NaN)\n","      else: \n","        return count, (sentiment['neg'],sentiment['pos'],sentiment['compound'])\n","    except:\n","      return np.NaN,(np.NaN,np.NaN,np.NaN)\n","\n","  def l_rc(self,soup,sentiment_analyser):\n","    try:\n","      content = soup.find('div',{'id':'risksAndChallenges'}).text\n","      sentiment = sentiment_analyser.polarity_scores(content)\n","      return self.count_words(content) - 8, (sentiment['neg'],sentiment['pos'],sentiment['compound'])\n","    except:\n","      return np.NaN,(np.NaN,np.NaN,np.NaN)\n","\n","  def l_env(self,soup,sentiment_analyser):\n","    try:\n","      content = soup.find('div',{'id':'environmentalCommitments'}).text\n","      sentiment = sentiment_analyser.polarity_scores(content)\n","      return self.count_words(content),(sentiment['neg'],sentiment['pos'],sentiment['compound'])\n","    except:\n","      return np.NaN,(np.NaN,np.NaN,np.NaN)\n","\n","  def save(self,scraped):\n","    print('Saving...')\n","    with jsonlines.open(self.data_destination, mode='a') as writer:\n","            writer.write_all(scraped)\n","    print('Save completed!')\n","\n","\n","  def count_words(self,text):\n","    tokens = re.findall(r'(\\d+(\\,\\d+)*(\\.?\\d+)*(st|\\'s|s|rd|th|nd)*|\\w+(\\-\\w+)*(\\'s)*)',text)\n","    return len(tokens)\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["time: 203 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Yu4ME8Jgpr8r","colab_type":"text"},"source":["# URL List"]},{"cell_type":"code","metadata":{"id":"M7pcH6TWZ1Zn","colab_type":"code","outputId":"8b87f2e0-e34e-4d69-80e5-5b709ace6d58","executionInfo":{"status":"ok","timestamp":1589272446288,"user_tz":-60,"elapsed":1243,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["with open('/content/drive/My Drive/Data Science Projects/Kickstarter/clean_data.csv','r') as file:\n","  urls = []\n","  for line in file.readlines()[99000:132000]:\n","    url = line.split('\\t')[24]\n","    if '?' in url:\n","      urls.append(url.split('?')[0])\n","    else:\n","      urls.append(url)\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["time: 800 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6qgfXRWe6IVX","colab_type":"code","outputId":"04a7e506-b2ad-4892-e317-05d6dccc5991","executionInfo":{"status":"ok","timestamp":1589272447946,"user_tz":-60,"elapsed":2486,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["file = '/content/drive/My Drive/Data Science Projects/Kickstarter/scraper/Scraper4/projects4.json'\n","scraper = DataScraper(urls,file)\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["9265 projects already stored. 9264 removed from list of URLs provided. 23736 projects remaining.\n","time: 1.69 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G3Q6VeRqbyd-","colab_type":"code","outputId":"baf2f30a-31af-43a0-fe5d-3ab63b45765e","executionInfo":{"status":"ok","timestamp":1588932500379,"user_tz":-60,"elapsed":76066,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["scraper.scrape()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["23736 to be scraped\n","50 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","100 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","150 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","200 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","250 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","300 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","350 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","400 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","450 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","500 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","550 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","600 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","650 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","700 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","750 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","800 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","850 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","900 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","950 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1000 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1050 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1100 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1150 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1200 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1250 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1300 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1350 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1400 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1450 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1500 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1550 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1600 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1650 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1700 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1750 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1800 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1850 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1900 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","1950 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","2000 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","2050 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","2100 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","2150 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","2200 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","2250 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n","2300 urls scraped since start. Saving...\n","Saving...\n","Save completed!\n"],"name":"stdout"}]}]}