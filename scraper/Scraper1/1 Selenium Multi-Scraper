{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1 Selenium Multi-Scraper","provenance":[],"mount_file_id":"1BxYP6fWk3rkT8EXS1H0lm2Xcu9NSKPXF","authorship_tag":"ABX9TyOuTKnZunS477DAwe3X4OOQ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Y-FeJsdyWMOQ","colab_type":"code","outputId":"33d0543b-10e7-4954-d191-a0fa840311a5","executionInfo":{"status":"ok","timestamp":1589292914382,"user_tz":-60,"elapsed":6184,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["#intalling chromedriver on computer instance \n","!apt install chromium-chromedriver\n","\n","#Installing selenium library\n","!pip install selenium"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","chromium-chromedriver is already the newest version (81.0.4044.122-0ubuntu0.18.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n","Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n","time: 5.67 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r6D94sCiYTBe","colab_type":"code","outputId":"39547fc5-4811-4e40-aa75-89a48e0587a8","executionInfo":{"status":"ok","timestamp":1589292914383,"user_tz":-60,"elapsed":6174,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#Importing webdriver and choosing settings compatible with Google Colab\n","from selenium import webdriver\n","chrome_options = webdriver.ChromeOptions()\n","chrome_options.add_argument('--headless')\n","chrome_options.add_argument('--no-sandbox')\n","chrome_options.add_argument('--disable-dev-shm-usage')\n","\n","#setting driver\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["time: 11.3 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Uou178p4sXBz","colab_type":"code","outputId":"f5a6ba95-50a5-4bad-9723-56a43343da07","executionInfo":{"status":"ok","timestamp":1589292924515,"user_tz":-60,"elapsed":16293,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# Other Libraries\n","from bs4 import BeautifulSoup\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import re\n","from time import sleep\n","!pip install vaderSentiment\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","!pip install jsonlines\n","import jsonlines\n","!pip install ipython-autotime\n","%load_ext autotime"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.6/dist-packages (3.3.1)\n","Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (1.2.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n","Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.6/dist-packages (0.1)\n","The autotime extension is already loaded. To reload it, use:\n","  %reload_ext autotime\n","time: 10.1 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-_ekmfWvtYu-","colab_type":"text"},"source":["# Scraper"]},{"cell_type":"code","metadata":{"id":"-VTVY1AxskT6","colab_type":"code","outputId":"b9bb8a31-756b-4a17-8ee3-7ffe51d84d6e","executionInfo":{"status":"ok","timestamp":1589292924516,"user_tz":-60,"elapsed":16284,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["class DataScraper:\n","  def __init__(self,list_of_urls,file_path):\n","    self.to_scrape = list_of_urls\n","    self.data_destination = file_path\n","    self.load()\n","\n","  def load(self): ##NEEDS UPDATING USING JSONLINES\n","    try:\n","      with jsonlines.open(file,'r') as reader:\n","        urls = 0\n","        found = 0\n","        for obj in reader:\n","          urls += 1\n","          key = list(obj.keys())[0]\n","          try:\n","            self.to_scrape.remove(key)\n","            found += 1\n","          except:\n","            pass\n","        print(f'{urls+1} projects already stored. {found} removed from list of URLs provided. {len(self.to_scrape)} projects remaining.')\n","    except:\n","      print(f'No previously recorded projects.{len(self.to_scrape)} projects remaining.')\n","\n","  def scrape(self):\n","    driver = webdriver.Chrome('chromedriver',options=chrome_options) #creates intance of webdriver\n","    sentiment_analyser = SentimentIntensityAnalyzer() #creates intance of vader sentiment analyser\n","    scraped = [] #creates empty list to later append individual {projects}\n","    errors = 0\n","    print(f'{len(self.to_scrape)} to be scraped') \n","    for x,url in enumerate(self.to_scrape): \n","      line = {url:{}} #empty project\n","      try:\n","        driver.get(url)\n","        tries = 2\n","        while tries > 0:\n","          sleep(2)\n","          soup = BeautifulSoup(driver.page_source, 'lxml')\n","          line[url]['r_levels'] = self.r_levels(soup)\n","          line[url]['n_videos'] = self.n_videos(soup)\n","          line[url]['n_imgs'], line[url]['n_gifs'] = self.imgs_and_gifs(soup)\n","          line[url]['story_l'],line[url]['story_s'] = self.l_story(soup,sentiment_analyser) #ADD EXCEPTION if len == 0!\n","          if np.isnan(line[url]['story_l']):\n","            tries = tries - 1\n","            if tries == 0:\n","              errors += 1\n","            continue\n","          line[url]['r&c_l'],line[url]['r&c_s'] = self.l_rc(soup,sentiment_analyser)\n","          line[url]['env_l'],line[url]['env_s'] = self.l_env(soup,sentiment_analyser)\n","          line[url]['faqs'] = self.get_faqs(soup)\n","          line[url]['updates'] = self.get_updates(soup)\n","          tries = 0\n","          scraped.append(line)\n","        if (x+1)%50 == 0:\n","          print(f'{x+1} urls scraped since start. {errors} errors. Saving...')\n","          self.save(scraped)\n","          scraped = []\n","          errors = 0\n","      except:\n","        print(f'{x+1} url/s scraped. Exception: Stopped at {url}')\n","    self.save(scraped)\n","    driver.close()\n","\n","    print(f'Scraping completed. {x+1} url/s scraped.')\n","\n","  def r_levels(self,soup):\n","    values = []\n","    try:\n","      reward_boxes = soup.find_all('div',{'class':'pledge__info'})\n","      for box in reward_boxes:\n","        try:\n","          value =re.search(r'\\d+',box.find('span',{'class':'money'}).text)\n","          values.append(value.group(0))\n","        except:\n","          pass\n","    except:\n","      pass\n","    return values\n","\n","  def n_videos(self,soup):\n","    videos = 0\n","    try:\n","      found = soup.find_all('video')\n","      videos += len(found)\n","    except:\n","      pass\n","    try:\n","      found = soup.find_all('iframe',{'class':\"embedly-card\"})\n","      videos += len(found)\n","    except:\n","      pass\n","    return videos\n","\n","  def get_faqs(self,soup):\n","    try:\n","      faqs = soup.find('a',{'id':'faq-emoji'}).get('emoji-data')\n","      return int(faqs)\n","    except:\n","      return np.NaN\n","\n","\n","  def get_updates(self,soup):\n","    try:\n","      faqs = soup.find('a',{'id':'updates-emoji'}).get('emoji-data')\n","      return int(faqs)\n","    except:\n","      return np.NaN\n","\n","  def imgs_and_gifs(self,soup):\n","    img = 0\n","    gif = 0\n","    try:\n","      media = soup.find_all('img')\n","      for element in media:\n","        if '.gif' in element.get('src').lower():\n","          gif += 1\n","        else:\n","          img += 1\n","    except:\n","      pass\n","    return img,gif\n","\n","  def l_story(self,soup,sentiment_analyser):\n","    try:\n","      content = soup.find('div',{'class':'rte__content'}).text\n","      sentiment = sentiment_analyser.polarity_scores(content)\n","      count = self.count_words(content)\n","      if count == 0:\n","        return np.NaN,(np.NaN,np.NaN,np.NaN)\n","      else: \n","        return count, (sentiment['neg'],sentiment['pos'],sentiment['compound'])\n","    except:\n","      return np.NaN,(np.NaN,np.NaN,np.NaN)\n","\n","  def l_rc(self,soup,sentiment_analyser):\n","    try:\n","      content = soup.find('div',{'id':'risksAndChallenges'}).text\n","      sentiment = sentiment_analyser.polarity_scores(content)\n","      return self.count_words(content) - 8, (sentiment['neg'],sentiment['pos'],sentiment['compound'])\n","    except:\n","      return np.NaN,(np.NaN,np.NaN,np.NaN)\n","\n","  def l_env(self,soup,sentiment_analyser):\n","    try:\n","      content = soup.find('div',{'id':'environmentalCommitments'}).text\n","      sentiment = sentiment_analyser.polarity_scores(content)\n","      return self.count_words(content),(sentiment['neg'],sentiment['pos'],sentiment['compound'])\n","    except:\n","      return np.NaN,(np.NaN,np.NaN,np.NaN)\n","\n","  def save(self,scraped):\n","    print('Saving...')\n","    with jsonlines.open(self.data_destination, mode='a') as writer:\n","            writer.write_all(scraped)\n","    print('Save completed!')\n","\n","\n","  def count_words(self,text):\n","    tokens = re.findall(r'(\\d+(\\,\\d+)*(\\.?\\d+)*(st|\\'s|s|rd|th|nd)*|\\w+(\\-\\w+)*(\\'s)*)',text)\n","    return len(tokens)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["time: 219 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Yu4ME8Jgpr8r","colab_type":"text"},"source":["# URL List"]},{"cell_type":"code","metadata":{"id":"M7pcH6TWZ1Zn","colab_type":"code","outputId":"79a70825-31a6-4ec1-ee39-fc08304abf24","executionInfo":{"status":"ok","timestamp":1589292925691,"user_tz":-60,"elapsed":17449,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["with open('/content/drive/My Drive/Data Science Projects/Kickstarter/Files/clean_data.csv','r') as file:\n","  urls = []\n","  for line in file.readlines()[1:33000]:\n","    url = line.split('\\t')[24]\n","    if '?' in url:\n","      urls.append(url.split('?')[0])\n","    else:\n","      urls.append(url)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["time: 916 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6qgfXRWe6IVX","colab_type":"code","outputId":"2226ae98-cd16-4ec4-de35-b87e7273e56d","executionInfo":{"status":"ok","timestamp":1589292931947,"user_tz":-60,"elapsed":23696,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["file = '/content/drive/My Drive/Data Science Projects/Kickstarter/Files/Scraping/projects1.json'\n","scraper = DataScraper(urls,file)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10010 projects already stored. 10005 removed from list of URLs provided. 22994 projects remaining.\n","time: 5.83 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G3Q6VeRqbyd-","colab_type":"code","outputId":"0f8161d9-7508-4f3d-e88d-6ec15b26e5ee","executionInfo":{"status":"ok","timestamp":1588932500379,"user_tz":-60,"elapsed":76066,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":442}},"source":["scraper.scrape()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["22994 to be scraped\n","50 urls scraped since start. 0 errors. Saving...\n","Saving...\n","Save completed!\n","100 urls scraped since start. 2 errors. Saving...\n","Saving...\n","Save completed!\n","150 urls scraped since start. 0 errors. Saving...\n","Saving...\n","Save completed!\n","200 urls scraped since start. 3 errors. Saving...\n","Saving...\n","Save completed!\n","250 urls scraped since start. 27 errors. Saving...\n","Saving...\n","Save completed!\n","300 urls scraped since start. 50 errors. Saving...\n","Saving...\n","Save completed!\n","350 urls scraped since start. 50 errors. Saving...\n","Saving...\n","Save completed!\n","400 urls scraped since start. 46 errors. Saving...\n","Saving...\n","Save completed!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yq_eRWB170nE","colab_type":"code","colab":{}},"source":["with jsonlines.open(file,'r') as reader:\n","  data = [x for x in reader]\n","\n","import numpy as np\n","\n","good = []\n","bad = 0\n","for project in data:\n","  if np.isnan(list(project.values())[0]['story_l']):\n","    bad += 1\n","  else:\n","    good.append(project)\n","\n","with jsonlines.open(file, mode='w') as writer:\n","            writer.write_all(good)\n","\n","print(len(good),bad)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mRpm8szWDW8-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}