{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2 Selenium Multi-Scraper","provenance":[],"toc_visible":true,"mount_file_id":"1LO0S_ShjdKI_7zZ4FJxsFEsG3J5CL1gw","authorship_tag":"ABX9TyPMyOhJ2m4h66joSqXIiFrb"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Y-FeJsdyWMOQ","colab_type":"code","outputId":"ff27c9d9-7640-4854-b1a3-bf3358e8dc27","executionInfo":{"status":"ok","timestamp":1589041691620,"user_tz":-60,"elapsed":6015,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["#intalling chromedriver on computer instance \n","!apt install chromium-chromedriver\n","\n","#Installing selenium library\n","!pip install selenium"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","chromium-chromedriver is already the newest version (81.0.4044.122-0ubuntu0.18.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n","Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n","time: 5.59 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r6D94sCiYTBe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8f422c1d-ea8a-4bef-9d0f-989d97d46e57","executionInfo":{"status":"ok","timestamp":1589041691621,"user_tz":-60,"elapsed":6003,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}}},"source":["#Importing webdriver and choosing settings compatible with Google Colab\n","from selenium import webdriver\n","chrome_options = webdriver.ChromeOptions()\n","chrome_options.add_argument('--headless')\n","chrome_options.add_argument('--no-sandbox')\n","chrome_options.add_argument('--disable-dev-shm-usage')\n","\n","#setting driver\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["time: 4.54 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Uou178p4sXBz","colab_type":"code","outputId":"33cc4e47-553c-426a-d2ff-a35d6459e46c","executionInfo":{"status":"ok","timestamp":1589041699569,"user_tz":-60,"elapsed":13861,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# Other Libraries\n","from bs4 import BeautifulSoup\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import re\n","from time import sleep\n","!pip install vaderSentiment\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","!pip install jsonlines\n","import jsonlines\n","!pip install ipython-autotime\n","%load_ext autotime"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.6/dist-packages (3.3.1)\n","Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (1.2.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n","Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.6/dist-packages (0.1)\n","The autotime extension is already loaded. To reload it, use:\n","  %reload_ext autotime\n","time: 7.98 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-_ekmfWvtYu-","colab_type":"text"},"source":["# Scraper"]},{"cell_type":"code","metadata":{"id":"-VTVY1AxskT6","colab_type":"code","outputId":"e2e11e09-6649-42b2-ceab-0e58912ad85c","executionInfo":{"status":"ok","timestamp":1589041699754,"user_tz":-60,"elapsed":13465,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["class DataScraper:\n","  def __init__(self,list_of_urls,file_path):\n","    self.to_scrape = list_of_urls\n","    self.data_destination = file_path\n","    self.load()\n","\n","  def load(self): ##NEEDS UPDATING USING JSONLINES\n","    try:\n","      with jsonlines.open(file,'r') as reader:\n","        urls = 0\n","        found = 0\n","        for obj in reader:\n","          urls += 1\n","          key = list(obj.keys())[0]\n","          try:\n","            self.to_scrape.remove(key)\n","            found += 1\n","          except:\n","            pass\n","        print(f'{urls+1} projects already stored. {found} removed from list of URLs provided. {len(self.to_scrape)} projects remaining.')\n","    except:\n","      print(f'No previously recorded projects.{len(self.to_scrape)} projects remaining.')\n","\n","  def scrape(self):\n","    driver = webdriver.Chrome('chromedriver',options=chrome_options) #creates intance of webdriver\n","    sentiment_analyser = SentimentIntensityAnalyzer() #creates intance of vader sentiment analyser\n","    scraped = [] #creates empty list to later append individual {projects}\n","    errors = 0\n","    print(f'{len(self.to_scrape)} to be scraped') \n","    for x,url in enumerate(self.to_scrape): \n","      line = {url:{}} #empty project\n","      try:\n","        driver.get(url)\n","        tries = 2\n","        while tries > 0:\n","          sleep(2)\n","          soup = BeautifulSoup(driver.page_source, 'lxml')\n","          line[url]['r_levels'] = self.r_levels(soup)\n","          line[url]['n_videos'] = self.n_videos(soup)\n","          line[url]['n_imgs'], line[url]['n_gifs'] = self.imgs_and_gifs(soup)\n","          line[url]['story_l'],line[url]['story_s'] = self.l_story(soup,sentiment_analyser) #ADD EXCEPTION if len == 0!\n","          if np.isnan(line[url]['story_l']):\n","            print('Oops')\n","            tries = tries - 1\n","            if tries == 0:\n","              errors += 1\n","            pass\n","          line[url]['r&c_l'],line[url]['r&c_s'] = self.l_rc(soup,sentiment_analyser)\n","          line[url]['env_l'],line[url]['env_s'] = self.l_env(soup,sentiment_analyser)\n","          line[url]['faqs'] = self.get_faqs(soup)\n","          line[url]['updates'] = self.get_updates(soup)\n","          tries = 0\n","          scraped.append(line)\n","        if (x+1)%50 == 0:\n","          print(f'{x+1} urls scraped since start. {errors} errors. Saving...')\n","          self.save(scraped)\n","          scraped = []\n","          errors = 0\n","      except:\n","        print(f'{x+1} url/s scraped. Exception: Stopped at {url}')\n","    self.save(scraped)\n","    driver.close()\n","\n","    print(f'Scraping completed. {x+1} url/s scraped.')\n","\n","  def r_levels(self,soup):\n","    values = []\n","    try:\n","      reward_boxes = soup.find_all('div',{'class':'pledge__info'})\n","      for box in reward_boxes:\n","        try:\n","          value =re.search(r'\\d+',box.find('span',{'class':'money'}).text)\n","          values.append(value.group(0))\n","        except:\n","          pass\n","    except:\n","      pass\n","    return values\n","\n","  def n_videos(self,soup):\n","    videos = 0\n","    try:\n","      found = soup.find_all('video')\n","      videos += len(found)\n","    except:\n","      pass\n","    try:\n","      found = soup.find_all('iframe',{'class':\"embedly-card\"})\n","      videos += len(found)\n","    except:\n","      pass\n","    return videos\n","\n","  def get_faqs(self,soup):\n","    try:\n","      faqs = soup.find('a',{'id':'faq-emoji'}).get('emoji-data')\n","      return int(faqs)\n","    except:\n","      return np.NaN\n","\n","\n","  def get_updates(self,soup):\n","    try:\n","      faqs = soup.find('a',{'id':'updates-emoji'}).get('emoji-data')\n","      return int(faqs)\n","    except:\n","      return np.NaN\n","\n","  def imgs_and_gifs(self,soup):\n","    img = 0\n","    gif = 0\n","    try:\n","      media = soup.find_all('img')\n","      for element in media:\n","        if '.gif' in element.get('src').lower():\n","          gif += 1\n","        else:\n","          img += 1\n","    except:\n","      pass\n","    return img,gif\n","\n","  def l_story(self,soup,sentiment_analyser):\n","    try:\n","      content = soup.find('div',{'class':'rte__content'}).text\n","      sentiment = sentiment_analyser.polarity_scores(content)\n","      count = self.count_words(content)\n","      if count == 0:\n","        return np.NaN,(np.NaN,np.NaN,np.NaN)\n","      else: \n","        return count, (sentiment['neg'],sentiment['pos'],sentiment['compound'])\n","    except:\n","      return np.NaN,(np.NaN,np.NaN,np.NaN)\n","\n","  def l_rc(self,soup,sentiment_analyser):\n","    try:\n","      content = soup.find('div',{'id':'risksAndChallenges'}).text\n","      sentiment = sentiment_analyser.polarity_scores(content)\n","      return self.count_words(content) - 8, (sentiment['neg'],sentiment['pos'],sentiment['compound'])\n","    except:\n","      return np.NaN,(np.NaN,np.NaN,np.NaN)\n","\n","  def l_env(self,soup,sentiment_analyser):\n","    try:\n","      content = soup.find('div',{'id':'environmentalCommitments'}).text\n","      sentiment = sentiment_analyser.polarity_scores(content)\n","      return self.count_words(content),(sentiment['neg'],sentiment['pos'],sentiment['compound'])\n","    except:\n","      return np.NaN,(np.NaN,np.NaN,np.NaN)\n","\n","  def save(self,scraped):\n","    print('Saving...')\n","    with jsonlines.open(self.data_destination, mode='a') as writer:\n","            writer.write_all(scraped)\n","    print('Save completed!')\n","\n","\n","  def count_words(self,text):\n","    tokens = re.findall(r'(\\d+(\\,\\d+)*(\\.?\\d+)*(st|\\'s|s|rd|th|nd)*|\\w+(\\-\\w+)*(\\'s)*)',text)\n","    return len(tokens)\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["time: 191 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Yu4ME8Jgpr8r","colab_type":"text"},"source":["# URL List"]},{"cell_type":"code","metadata":{"id":"M7pcH6TWZ1Zn","colab_type":"code","outputId":"4946b895-907c-4e3b-91fb-a88428f86e15","executionInfo":{"status":"ok","timestamp":1589041700899,"user_tz":-60,"elapsed":13818,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["with open('/content/drive/My Drive/Data Science Projects/Kickstarter/clean_data.csv','r') as file:\n","  urls = []\n","  for line in file.readlines()[33000:66000]:\n","    url = line.split('\\t')[24]\n","    if '?' in url:\n","      urls.append(url.split('?')[0])\n","    else:\n","      urls.append(url)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["time: 1.01 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6qgfXRWe6IVX","colab_type":"code","outputId":"0108f5bf-4d23-49a7-f06e-0fe0310978f9","executionInfo":{"status":"ok","timestamp":1589041701094,"user_tz":-60,"elapsed":13732,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["file = '/content/drive/My Drive/Data Science Projects/Kickstarter/scraper/Scraper2/projects2.json'\n","scraper = DataScraper(urls,file)\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["13601 projects already stored. 13600 removed from list of URLs provided. 19400 projects remaining.\n","time: 219 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G3Q6VeRqbyd-","colab_type":"code","outputId":"fe07ea5a-6d8f-4103-ee2b-5d68c339eeb0","executionInfo":{"status":"ok","timestamp":1588932500379,"user_tz":-60,"elapsed":76066,"user":{"displayName":"Hern치n Roberto Nowak","photoUrl":"","userId":"15487082726057573294"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["scraper.scrape()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["19400 to be scraped\n","Oops\n","50 urls scraped since start. 0 errors. Saving...\n","Saving...\n","Save completed!\n","100 urls scraped since start. 0 errors. Saving...\n","Saving...\n","Save completed!\n","Oops\n","150 urls scraped since start. 0 errors. Saving...\n","Saving...\n","Save completed!\n","Oops\n","200 urls scraped since start. 0 errors. Saving...\n","Saving...\n","Save completed!\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","250 urls scraped since start. 0 errors. Saving...\n","Saving...\n","Save completed!\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","300 urls scraped since start. 0 errors. Saving...\n","Saving...\n","Save completed!\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","350 urls scraped since start. 0 errors. Saving...\n","Saving...\n","Save completed!\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","Oops\n","400 urls scraped since start. 0 errors. Saving...\n","Saving...\n","Save completed!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r3cTcUOzDpCL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}